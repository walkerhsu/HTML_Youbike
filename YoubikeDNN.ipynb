{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7086646,"sourceType":"datasetVersion","datasetId":4083080},{"sourceId":7093039,"sourceType":"datasetVersion","datasetId":4087683},{"sourceId":7093178,"sourceType":"datasetVersion","datasetId":4087799}],"dockerImageVersionId":30587,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Install Packages ","metadata":{}},{"cell_type":"code","source":"# !pip install scikit-learn\n# !pip install numpy\n# !pip install pandas\n# !pip install torch\n# !pip install tqdm","metadata":{"execution":{"iopub.status.busy":"2023-11-30T18:51:49.305786Z","iopub.execute_input":"2023-11-30T18:51:49.306599Z","iopub.status.idle":"2023-11-30T18:51:49.310864Z","shell.execute_reply.started":"2023-11-30T18:51:49.306556Z","shell.execute_reply":"2023-11-30T18:51:49.309912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Import Packages","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n\nfrom tqdm import tqdm\nimport math\nimport random\nimport csv","metadata":{"_uuid":"c82b120e-39f1-4533-9688-3a0b859a062f","_cell_guid":"3886c10b-f7c4-47cf-8857-2f30cdcedb45","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-11-30T18:51:49.312810Z","iopub.execute_input":"2023-11-30T18:51:49.313403Z","iopub.status.idle":"2023-11-30T18:51:49.323658Z","shell.execute_reply.started":"2023-11-30T18:51:49.313367Z","shell.execute_reply":"2023-11-30T18:51:49.322814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Useful Functions ","metadata":{}},{"cell_type":"markdown","source":"### Set seeds","metadata":{}},{"cell_type":"code","source":"def same_seeds(seed):\n    random.seed(seed) \n    np.random.seed(seed)  \n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed) \n    torch.backends.cudnn.benchmark = False\n    torch.backends.cudnn.deterministic = True","metadata":{"execution":{"iopub.status.busy":"2023-11-30T18:51:49.325554Z","iopub.execute_input":"2023-11-30T18:51:49.325898Z","iopub.status.idle":"2023-11-30T18:51:49.333977Z","shell.execute_reply.started":"2023-11-30T18:51:49.325867Z","shell.execute_reply":"2023-11-30T18:51:49.333145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Read Dataset from CSV Files","metadata":{}},{"cell_type":"code","source":"def readDataset(data_filepath, inference_filepath):\n    assert os.path.exists(data_filepath)\n    filenames = os.listdir(data_filepath)\n    if '.DS_Store' in filenames:\n        filenames.remove('.DS_Store')\n    filenames = sorted(filenames)\n    train, valid = [], []\n    \n    for idx, filename in enumerate(filenames):\n        data = (pd.read_csv(data_filepath + filename).values).tolist()\n        if idx >= len(filenames) - 14:\n            # validation\n            for single_data in data:\n                valid.append(single_data)\n        else:\n            #training\n            for single_data in data:\n                train.append(single_data)\n    \n    assert os.path.exists(inference_filepath)\n    filenames = os.listdir(inference_filepath)\n    if '.DS_Store' in filenames:\n        filenames.remove('.DS_Store')\n    filenames = sorted(filenames, reverse=True)\n    test = []\n    for idx, filename in enumerate(filenames):\n        print(filename)\n        data = (pd.read_csv(inference_filepath + filename).values).tolist()\n        for single_data in data:\n            test.append(single_data)\n    return train, valid, test","metadata":{"_uuid":"34f64581-b863-4aed-90cf-714ec8edcd5e","_cell_guid":"590ecbce-8877-4ca8-ae7f-d7d83992502b","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-11-30T18:51:49.335301Z","iopub.execute_input":"2023-11-30T18:51:49.335568Z","iopub.status.idle":"2023-11-30T18:51:49.345276Z","shell.execute_reply.started":"2023-11-30T18:51:49.335544Z","shell.execute_reply":"2023-11-30T18:51:49.344426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## My Youbike Dataset Class ","metadata":{}},{"cell_type":"code","source":"class YoubikeDataset(Dataset):\n    def __init__(self, data, dataType):\n        super(YoubikeDataset, self).__init__()\n        # [month, date, weekday, hr, min, lat, lng, act, ratio, sbi, tot, title, act_title]\n        # [month, day ,weekday ,hr ,min ,lat ,lng ,act ,tot ,title ]\n        self.data = data\n        self.datasize = len(self.data)\n        self.type = dataType\n\n    def __getitem__(self, idx):\n        features = self.data[idx][:8]\n        if self.type == \"train\" or self.type == \"val\":\n            label = [self.data[idx][8]]\n            return torch.FloatTensor(features), torch.FloatTensor(label)\n        elif self.type == \"test\":\n            total = self.data[idx][8]\n            title = self.data[idx][-1]\n            return torch.FloatTensor(features), total, title\n        else:\n            raise NotImplementedError\n            \n    def __len__(self):\n        return self.datasize","metadata":{"_uuid":"24f46a6f-ebb4-4cab-b211-834442d0d02e","_cell_guid":"d1f743cb-4af8-40f9-8491-c97b92210c97","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-11-30T18:51:49.346340Z","iopub.execute_input":"2023-11-30T18:51:49.346591Z","iopub.status.idle":"2023-11-30T18:51:49.358438Z","shell.execute_reply.started":"2023-11-30T18:51:49.346569Z","shell.execute_reply":"2023-11-30T18:51:49.357603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## My Model(s)","metadata":{}},{"cell_type":"markdown","source":"### DNN model","metadata":{}},{"cell_type":"code","source":"class My_Model(nn.Module):\n    def __init__(self, input_dim):\n        super(My_Model, self).__init__()\n        # TODO: modify model's structure, be aware of dimensions. \n        self.layers = nn.Sequential(\n            nn.Linear(input_dim, 2*input_dim),\n#             nn.Dropout(p=0.1),\n            nn.Sigmoid(),\n            nn.Linear(2*input_dim, 4*input_dim),\n#             nn.Dropout(p=0.1),\n            nn.Sigmoid(),\n            nn.Linear(4*input_dim, 2*input_dim),\n#             nn.Dropout(p=0.1),\n            nn.Sigmoid(),\n            nn.Linear(2*input_dim, input_dim),\n#             nn.Dropout(p=0.1),\n            nn.Sigmoid(),\n            nn.Linear(input_dim, 4),\n            nn.Sigmoid(),\n            nn.Linear(4, 2),\n            nn.Sigmoid(),\n            nn.Linear(2, 1),\n            nn.Sigmoid(),\n        )\n\n    def forward(self, x):\n        x = self.layers(x)\n        return x","metadata":{"_uuid":"8064e40a-7ebd-4adc-a871-8786bb42c7b1","_cell_guid":"c9b009bb-f67b-4740-9bf0-14c4c57c597c","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-11-30T18:51:49.360058Z","iopub.execute_input":"2023-11-30T18:51:49.360303Z","iopub.status.idle":"2023-11-30T18:51:49.369294Z","shell.execute_reply.started":"2023-11-30T18:51:49.360278Z","shell.execute_reply":"2023-11-30T18:51:49.368485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## My Loss Function","metadata":{}},{"cell_type":"code","source":"def getLoss(pred, label):\n    loss = torch.mean(3*torch.abs(pred-label)*(torch.abs(pred-1/3)+torch.abs(pred-2/3)))\n#     print(f\"loss = {loss}\")\n    return loss","metadata":{"execution":{"iopub.status.busy":"2023-11-30T18:51:49.370245Z","iopub.execute_input":"2023-11-30T18:51:49.370483Z","iopub.status.idle":"2023-11-30T18:51:49.384311Z","shell.execute_reply.started":"2023-11-30T18:51:49.370462Z","shell.execute_reply":"2023-11-30T18:51:49.383503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training function","metadata":{}},{"cell_type":"code","source":"def train(model, config, train_loader, valid_loader, device):\n#     criterion = nn.MSELoss(reduction='mean') # Define your loss function, do not modify this.\n    # criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.SGD(model.parameters(), lr=config['learning_rate'], weight_decay=config['weight_decay']) \n    scheduler = CosineAnnealingWarmRestarts(optimizer,T_0=1,T_mult=2)\n    if not os.path.isdir(config[\"save_dir\"]):\n        os.mkdir(config[\"save_dir\"]) # Create directory of saving models.\n\n    n_epochs, best_loss, step, early_stop_count = config['epochs'], math.inf, 0, 0\n\n    for epoch in range(n_epochs):\n        model.train() # Set your model to train mode.\n        loss_record = []\n        print(scheduler.get_last_lr())\n\n        # tqdm is a package to visualize your training progress.\n        train_pbar = tqdm(train_loader, position=0, leave=True)\n\n        for x, y in train_pbar:\n            optimizer.zero_grad()               # Set gradient to zero.\n            x, y = x.to(device), y.to(device)   # Move your data to device. \n            pred = model(x) \n            loss = getLoss(pred, y)\n            loss.backward()                     # Compute gradient(backpropagation).\n            optimizer.step()                    # Update parameters.\n            step += 1\n            loss_record.append(loss.detach().item())\n            \n            # Display current epoch number and loss on tqdm progress bar.\n            train_pbar.set_description(f'Epoch [{epoch+1}/{n_epochs}]')\n            train_pbar.set_postfix({'loss': loss.detach().item()})\n\n        scheduler.step()\n        mean_train_loss = sum(loss_record)/len(loss_record)\n\n        model.eval() # Set your model to evaluation mode.\n        loss_record = []\n        for x, y in valid_loader:\n            x, y = x.to(device), y.to(device)\n            with torch.no_grad():\n                pred = model(x)\n                loss = getLoss(pred, y)\n\n            loss_record.append(loss.item())\n            \n        mean_valid_loss = sum(loss_record)/len(loss_record)\n        print(f'Epoch [{epoch+1}/{n_epochs}]: Train loss: {mean_train_loss:.4f}, Valid loss: {mean_valid_loss:.4f}')\n        # writer.add_scalar('Loss/valid', mean_valid_loss, step)\n\n        if mean_valid_loss < best_loss:\n            best_loss = mean_valid_loss\n            torch.save(model.state_dict(), config['save_dir'] + config['model_name']) # Save your best model\n            print('Saving model with loss {:.3f}...'.format(best_loss))\n            early_stop_count = 0\n        else: \n            early_stop_count += 1\n\n        if early_stop_count >= config['early_stop']:\n            print('\\nModel is not improving, so we halt the training session.')\n            print('best loss {:.3f}...'.format(best_loss))\n            return","metadata":{"_uuid":"86c145bf-8323-4e9b-9896-fd1a6fa34695","_cell_guid":"8eb27ae9-f1a0-48ec-b592-b30bbd58c010","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-11-30T18:51:49.459516Z","iopub.execute_input":"2023-11-30T18:51:49.459796Z","iopub.status.idle":"2023-11-30T18:51:49.472699Z","shell.execute_reply.started":"2023-11-30T18:51:49.459773Z","shell.execute_reply":"2023-11-30T18:51:49.471661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Predict function","metadata":{}},{"cell_type":"code","source":"def predict(test_loader, model, device):\n    model.eval() # Set your model to evaluation mode.\n    preds = []\n    tots = []\n    titles = []\n    for x, tot, title in tqdm(test_loader):\n        tot = tot.tolist()\n        x = x.to(device)      \n        tots = tots + tot\n        titles = titles + title\n        with torch.no_grad():                   \n            pred = model(x)                     \n            preds.append(pred.detach().cpu())\n            \n    preds = torch.cat(preds, dim=0).numpy().tolist()\n    print(len(preds), len(tots), len(titles))\n    assert len(preds) == len(tots)\n    assert len(tots) == len(titles)\n    prediction = [['id','sbi']]\n    for (pred, tot, title) in zip(preds, tots, titles):\n        \n        prediction.append([title, pred[0]*tot])\n    \n    with open('prediction.csv', 'w', newline='') as file:\n    # Step 4: Using csv.writer to write the list to the CSV file\n        writer = csv.writer(file)\n        writer.writerows(prediction) # Use writerows for nested list\n    \n    return","metadata":{"execution":{"iopub.status.busy":"2023-11-30T18:51:49.474349Z","iopub.execute_input":"2023-11-30T18:51:49.474611Z","iopub.status.idle":"2023-11-30T18:51:49.485960Z","shell.execute_reply.started":"2023-11-30T18:51:49.474588Z","shell.execute_reply":"2023-11-30T18:51:49.485231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Hyperparameters","metadata":{}},{"cell_type":"code","source":"config = {\n    \"batch_size\": 32,\n#     \"data_filepath\": 'dataset_csv/',\n    \"data_filepath\": '/kaggle/input/dataset-1201/dataset_csv/',\n    \"inference_filepath\": '/kaggle/input/inference-1204/inference_csv/',\n    \"epochs\": 50,\n    \"learning_rate\": 1,\n    \"weight_decay\": 5e-3,\n    \"save_dir\": \"./models/\",\n    \"model_name\": \"1201-DNN.ckpt\",\n    \"early_stop\":50,\n    \"seeds\": 10901036\n}\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'","metadata":{"_uuid":"1642af7c-4541-401c-97cd-fd4aed575e53","_cell_guid":"050181da-4d1e-4cce-addc-868287bbf465","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-11-30T18:52:19.997123Z","iopub.execute_input":"2023-11-30T18:52:19.998026Z","iopub.status.idle":"2023-11-30T18:52:20.004453Z","shell.execute_reply.started":"2023-11-30T18:52:19.997976Z","shell.execute_reply":"2023-11-30T18:52:20.003389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load Datasets","metadata":{}},{"cell_type":"code","source":"same_seeds(config[\"seeds\"])\ntrain_data, valid_data, test_data = readDataset(config['data_filepath'], config['inference_filepath'])\nprint(f'train_data_size: {len(train_data)}')\nprint(f'valid_data_size: {len(valid_data)}')\nprint(f'test_data_size : {len(test_data)}')\ntrain_dataset, valid_dataset, test_dataset = YoubikeDataset(train_data, \"train\"), YoubikeDataset(valid_data, \"val\"), YoubikeDataset(test_data, \"test\")\ntrain_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True, pin_memory=True)\nvalid_loader = DataLoader(valid_dataset, batch_size=config['batch_size'], shuffle=True, pin_memory=True)\ntest_loader  = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=False, pin_memory=True)","metadata":{"_uuid":"c80b8a6f-f869-40d3-9822-05a3747b9ceb","_cell_guid":"58d5efaf-76ee-4332-9085-e6d1b623cfc5","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-11-30T18:52:20.006327Z","iopub.execute_input":"2023-11-30T18:52:20.006710Z","iopub.status.idle":"2023-11-30T18:52:21.973835Z","shell.execute_reply.started":"2023-11-30T18:52:20.006677Z","shell.execute_reply":"2023-11-30T18:52:21.972911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Start Training","metadata":{}},{"cell_type":"code","source":"model = My_Model(input_dim=8).to(device) # put your model and data on the same computation device.\n\ntrain(model, config, train_loader, valid_loader, device)","metadata":{"_uuid":"0c61441f-52c7-4ecc-a3ac-d8c75362120a","_cell_guid":"192b0a7e-9672-43b9-ae6c-b01f7f217a2f","execution":{"iopub.status.busy":"2023-11-30T18:52:21.975443Z","iopub.execute_input":"2023-11-30T18:52:21.975838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Inference","metadata":{}},{"cell_type":"code","source":"predict(test_loader, model, device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}